---
title: "ANLY590AS0"
author: "Wupeng Han"
date: "2018/9/8"
output:
  html_document: default
  pdf_document: default
---
#1

## 1.1
```{r}
library(glmnet)
df=read.csv("Hitters.csv")
#select numeric features
df=df[c(2:14,17:20)]
#drop missing value
df<-na.omit(df)

x=model.matrix(Salary~., data= df)
y=df$Salary
fit.lasso<-glmnet(x,y,alpha=1)
#1.1.1
plot(fit.lasso,xvar="lambda",label=TRUE,main="Standardized")
predict(fit.lasso,s=180, type = "coefficients")


```
##1.1.2
We can see the final three predictors that remain in the model are "Hits" "CRuns"and "CRBI", which means those three are the most important features to predict the predict the Hitters' salary.

## 1.1.3 & 1.1.4 

```{r}
set.seed(123)
cv.lasso<-cv.glmnet(x,y, alpha =1)
plot(cv.lasso)
#1.1.3
bl<-cv.lasso$lambda.min
print("The optimal value of the regularization penalty:")
bl
#1.1.4
predict(fit.lasso,s=bl, type = "coefficients")


```

There are 13 predictors are left in the model.

# 1.2
## 1.2.1
```{r}
fit.ridge=glmnet(x,y,alpha=0)
plot(fit.ridge,xvar="lambda",label=TRUE,main="Standardized")
```

##1.2.2
```{r}
set.seed(123)
cv.ridge<-cv.glmnet(x,y, alpha =0)
plot(cv.ridge)
print("The optimal value of ridge's regularization penalty:")
cv.ridge$lambda.min
```


#2

#2.1

The bias-variance tradeoff is the property of a set of predictive models whereby models with a lower bias in parameter estimation have a higher variance of the parameter estimates across samples, and vice versa.

#2.2
Regularization methods introduce bias into the regression solution that can reduce variance considerably relative to the ordinary least squares (OLS) solution. Although the OLS solution provides non-biased regression estimates, the lower variance solutions produced by regularization techniques provide superior MSE performance.

#2.3

In the ridge and lasso models, we can find the regularization parameter lambda controls the penalty strength.Larger lambda leads to smaller coefficients which means a higher bias.  





